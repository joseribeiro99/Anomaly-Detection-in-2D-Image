{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Size of our input images\n",
    "SIZE = 128\n",
    "\n",
    "#############################################################################\n",
    "#Define generators for training, validation and also anomaly data.\n",
    "\n",
    "batch_size = 64\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    'cell_images2/uninfected_train/',\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    'cell_images2/uninfected_test/',\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )\n",
    "\n",
    "anomaly_generator = datagen.flow_from_directory(\n",
    "    'cell_images2/parasitized/',\n",
    "    target_size=(SIZE, SIZE),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='input'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the autoencoder. \n",
    "#Try to make the bottleneck layer size as small as possible to make it easy for\n",
    "#density calculations and also picking appropriate thresholds. \n",
    "\n",
    "#Encoder\n",
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3)))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "\n",
    "#Decoder\n",
    "model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
    "model.add(UpSampling2D((2, 2)))\n",
    "\n",
    "model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model. \n",
    "history = model.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch= 500 // batch_size,\n",
    "        epochs=1000,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=75 // batch_size,\n",
    "        shuffle = True)\n",
    "\n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all batches generated by the datagen and pick a batch for prediction\n",
    "#Just to test the model. \n",
    "data_batch = []  #Capture all training batches as a numpy array\n",
    "img_num = 0\n",
    "while img_num <= train_generator.batch_index:   #gets each generated batch of size batch_size\n",
    "    data = train_generator.next()\n",
    "    data_batch.append(data[0])\n",
    "    img_num = img_num + 1\n",
    "\n",
    "predicted = model.predict(data_batch[0])  #Predict on the first batch of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sanity check, view few images and corresponding reconstructions\n",
    "image_number = random.randint(0, predicted.shape[0])\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(data_batch[0][image_number])\n",
    "plt.subplot(122)\n",
    "plt.imshow(predicted[image_number])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us examine the reconstruction error between our validation data (good/normal images)\n",
    "# and the anomaly images\n",
    "validation_error = model.evaluate_generator(validation_generator)\n",
    "anomaly_error = model.evaluate_generator(anomaly_generator)\n",
    "\n",
    "print(\"Recon. error for the validation (normal) data is: \", validation_error)\n",
    "print(\"Recon. error for the anomaly data is: \", anomaly_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us extract (or build) the encoder network, with trained weights.\n",
    "#This is used to get the compressed output (latent space) of the input image. \n",
    "#The compressed output is then used to calculate the KDE\n",
    "\n",
    "encoder_model = Sequential()\n",
    "encoder_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3), weights=model.layers[0].get_weights()) )\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "encoder_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', weights=model.layers[2].get_weights()))\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "encoder_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', weights=model.layers[4].get_weights()))\n",
    "encoder_model.add(MaxPooling2D((2, 2), padding='same'))\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "# Calculate KDE using sklearn\n",
    "from sklearn.neighbors import KernelDensity\n",
    "\n",
    "#Get encoded output of input images = Latent space\n",
    "encoded_images = encoder_model.predict_generator(train_generator)\n",
    "\n",
    "# Flatten the encoder output because KDE from sklearn takes 1D vectors as input\n",
    "encoder_output_shape = encoder_model.output_shape #Here, we have 16x16x16\n",
    "out_vector_shape = encoder_output_shape[1]*encoder_output_shape[2]*encoder_output_shape[3]\n",
    "\n",
    "encoded_images_vector = [np.reshape(img, (out_vector_shape)) for img in encoded_images]\n",
    "\n",
    "#Fit KDE to the image latent data\n",
    "kde = KernelDensity(kernel='gaussian', bandwidth=0.2).fit(encoded_images_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate density and reconstruction error to find their means values for\n",
    "#good and anomaly images. \n",
    "#We use these mean and sigma to set thresholds. \n",
    "def calc_density_and_recon_error(batch_images):\n",
    "    \n",
    "    density_list=[]\n",
    "    recon_error_list=[]\n",
    "    for im in range(0, batch_images.shape[0]-1):\n",
    "        \n",
    "        img  = batch_images[im]\n",
    "        img = img[np.newaxis, :,:,:]\n",
    "        encoded_img = encoder_model.predict([[img]]) # Create a compressed version of the image using the encoder\n",
    "        encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img] # Flatten the compressed image\n",
    "        density = kde.score_samples(encoded_img)[0] # get a density score for the new image\n",
    "        reconstruction = model.predict([[img]])\n",
    "        reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]\n",
    "        density_list.append(density)\n",
    "        recon_error_list.append(reconstruction_error)\n",
    "        \n",
    "    average_density = np.mean(np.array(density_list))  \n",
    "    stdev_density = np.std(np.array(density_list)) \n",
    "    \n",
    "    average_recon_error = np.mean(np.array(recon_error_list))  \n",
    "    stdev_recon_error = np.std(np.array(recon_error_list)) \n",
    "    \n",
    "    return average_density, stdev_density, average_recon_error, stdev_recon_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get average and std dev. of density and recon. error for uninfected and anomaly (parasited) images. \n",
    "#For this let us generate a batch of images for each. \n",
    "train_batch = train_generator.next()[0]\n",
    "anomaly_batch = anomaly_generator.next()[0]\n",
    "\n",
    "uninfected_values = calc_density_and_recon_error(train_batch)\n",
    "anomaly_values = calc_density_and_recon_error(anomaly_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, input unknown images and sort as Good or Anomaly\n",
    "def check_anomaly(img_path):\n",
    "    density_threshold = 2500 #Set this value based on the above exercise\n",
    "    reconstruction_error_threshold = 0.004 # Set this value based on the above exercise\n",
    "    img  = Image.open(img_path)\n",
    "    img = np.array(img.resize((128,128), Image.ANTIALIAS))\n",
    "    plt.imshow(img)\n",
    "    img = img / 255.\n",
    "    img = img[np.newaxis, :,:,:]\n",
    "    encoded_img = encoder_model.predict([[img]]) \n",
    "    encoded_img = [np.reshape(img, (out_vector_shape)) for img in encoded_img] \n",
    "    density = kde.score_samples(encoded_img)[0] \n",
    "\n",
    "    reconstruction = model.predict([[img]])\n",
    "    reconstruction_error = model.evaluate([reconstruction],[[img]], batch_size = 1)[0]\n",
    "\n",
    "    if density < density_threshold or reconstruction_error > reconstruction_error_threshold:\n",
    "        print(\"The image is an anomaly\")\n",
    "        \n",
    "    else:\n",
    "        print(\"The image is NOT an anomaly\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load a couple of test images and verify whether they are reported as anomalies.\n",
    "import glob\n",
    "para_file_paths = glob.glob('cell_images2/parasitized/images/*')\n",
    "uninfected_file_paths = glob.glob('cell_images2/uninfected_train/images/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anomaly image verification\n",
    "num=random.randint(0,len(para_file_paths)-1)\n",
    "check_anomaly(para_file_paths[num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Good/normal image verification\n",
    "num=random.randint(0,len(para_file_paths)-1)\n",
    "check_anomaly(uninfected_file_paths[num])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('AD')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1e50bde906a6bc0a915c451f11013671639285bcbf2b9acf015abdaa286607c3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
